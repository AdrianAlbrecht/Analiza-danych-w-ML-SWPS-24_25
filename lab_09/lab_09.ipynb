{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09 - Inżynieria cech. Selekcja cech. PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inżynieria cech** (ang. *feature engineering*) to proces tworzenia, modyfikowania i przekształcania danych wejściowych w sposób, który maksymalizuje wydajność modelu uczenia maszynowego. Dobre cechy mogą znacząco poprawić zdolność modelu do przewidywania wyników, podczas gdy złe cechy mogą prowadzić do niskiej jakości predykcji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cel i znaczenie inżynierii cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane surowe, z którymi często pracujemy, mogą nie być w idealnej formie do uczenia maszynowego. Mogą zawierać szum, brakujące wartości lub być w formatach, które modele nie są w stanie przetworzyć (np. dane tekstowe). Inżynieria cech pozwala na:\n",
    "\n",
    "1. Przygotowanie danych w sposób zgodny z wymaganiami modelu.\n",
    "2. Poprawę reprezentacji danych poprzez tworzenie nowych cech na podstawie istniejących (np. obliczenie średniej z kolumn).\n",
    "3. Zmniejszenie wymiarowości danych poprzez redukcję zbędnych informacji.\n",
    "4. Podniesienie jakości danych poprzez usuwanie szumu lub standaryzację wartości.\n",
    "\n",
    "Dobrze zaprojektowane cechy mogą być kluczowe w przypadku mniej zaawansowanych algorytmów lub danych z dużą ilością szumu. Często mówi się, że:\n",
    "\n",
    "            \"Jakość modelu zależy w 80% od jakości cech, a tylko w 20% od algorytmu\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kluczowe etapy inżynierii cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tworzenie cech (ang. feature creation)\n",
    "\n",
    "Tworzenie nowych cech na podstawie istniejących danych. Na przykład:\n",
    "\n",
    "* Obliczenie stosunku dwóch zmiennych (np. ceny do liczby pokoi w analizie rynku nieruchomości).\n",
    "* Ekstrakcja cech z danych czasowych (np. dzień tygodnia, godzina).\n",
    "* Tworzenie cech logicznych (np. \"czy użytkownik dokonał zakupu?\").\n",
    "\n",
    "2. Przekształcanie cech (ang. feature transformation)\n",
    "\n",
    "Przekształcenie danych w sposób, który ułatwia ich analizę. Obejmuje:\n",
    "\n",
    "* Skalowanie: Przekształcenie wartości do tego samego zakresu (np. standaryzacja lub normalizacja).\n",
    "* Kodowanie zmiennych kategorycznych: Zamiana tekstowych etykiet na wartości numeryczne (np. one-hot encoding).\n",
    "* Logarytmowanie: Zmniejszenie wpływu dużych wartości w danych.\n",
    "\n",
    "3. Obsługa brakujących danych\n",
    "* Uzupełnianie brakujących wartości (np. średnią, medianą).\n",
    "* Usuwanie kolumn z dużą ilością brakujących danych.\n",
    "\n",
    "4. **Redukcja cech**\n",
    "\n",
    "Redukowanie liczby cech poprzez analizę zmienności (np. PCA - analiza głównych składowych)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czym jest selekcja cech?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selekcja cech** (ang. *feature selection*) to proces wyboru najistotniejszych cech (zmiennych, kolumn) spośród dostępnych w zbiorze danych. Celem tego procesu jest identyfikacja tych cech, które mają największy wpływ na wynik modelu predykcyjnego, przy jednoczesnym wyeliminowaniu tych, które są mniej istotne, redundantne lub nieistotne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cel i zastosowanie selekcji cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W praktyce, zbiory danych mogą zawierać setki, a nawet tysiące cech, co prowadzi do problemów takich jak:\n",
    "\n",
    "1. Wysoka wymiarowość danych:\n",
    " * Każda dodatkowa cecha zwiększa wymiar przestrzeni, w której działa model. Wysoka wymiarowość prowadzi do tzw. klątwy wymiarowości – trudności w znalezieniu właściwych wzorców w danych.\n",
    " \n",
    "2. Zwiększone ryzyko przeuczenia (overfitting):\n",
    "* Model może nauczyć się szumu w danych, co prowadzi do gorszych wyników na nowych, nieznanych danych.\n",
    "\n",
    "3. Wysokie koszty obliczeniowe:\n",
    "* Praca z dużymi zbiorami danych wymaga większej mocy obliczeniowej, więcej pamięci, a proces uczenia się modelu jest dłuższy.\n",
    "\n",
    "4. Złożoność interpretacji:\n",
    "* Modele z mniejszą liczbą cech są łatwiejsze do zrozumienia i wyjaśnienia, co jest istotne np. w medycynie czy finansach, gdzie ważna jest interpretowalność decyzji.\n",
    "\n",
    "Selekcja cech pomaga w:\n",
    "* Zwiększeniu wydajności modeli poprzez usunięcie nieistotnych danych.\n",
    "* Poprawie generalizacji na nowych danych dzięki zmniejszeniu ryzyka przeuczenia.\n",
    "* Przyspieszeniu obliczeń.\n",
    "* Ułatwieniu interpretacji modeli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rodzaje selekcji cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Metody filtrujące (ang. filter methods)\n",
    "* Wykorzystują miary statystyczne, aby ocenić związek między każdą cechą a celem predykcji.\n",
    "* Zalety: Szybkie i niezależne od modelu.\n",
    "* Przykłady metod:\n",
    "    * Korelacja Pearson: Do danych ciągłych.\n",
    "    * Test chi-kwadrat: Do danych kategorycznych.\n",
    "    * ANOVA: Do porównania różnych grup.\n",
    "\n",
    "2. Metody osadzone (ang. embedded methods)\n",
    "* Wbudowane w algorytm uczenia się. Podczas trenowania modelu określają, które cechy są najważniejsze.\n",
    "* Przykłady:\n",
    "    * Regresja Lasso: Dodaje karę za współczynniki cech nieistotnych, zmuszając je do zerowania.\n",
    "    * Random Forest: Oblicza znaczenie cech na podstawie zmniejszenia błędu przy podziale drzewa.\n",
    "\n",
    "3. Metody wrapperowe (ang. wrapper methods)\n",
    "* Testują różne podzbiory cech, aby znaleźć ten, który daje najlepsze wyniki modelu.\n",
    "* Przykłady:\n",
    "    * Recursive Feature Elimination (RFE): Iteracyjnie usuwa najmniej istotne cechy.\n",
    "    * Metody oparte na optymalizacji (np. algorytmy genetyczne).\n",
    "* Wady: Wysoka złożoność obliczeniowa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykład praktyczny: Czy selekcja cech jest zawsze konieczna?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyobraźmy sobie model predykcyjny przewidujący, czy pacjent ma określoną chorobę, bazując na zbiorze danych o 100 cechach, takich jak:\n",
    "* wiek,\n",
    "* płeć,\n",
    "* masa ciała,\n",
    "* poziom glukozy we krwi,\n",
    "* ciśnienie krwi,\n",
    "* wyniki 90 różnych testów laboratoryjnych.\n",
    "\n",
    "Bez selekcji cech model może próbować dopasować się do szumu w wynikach testów laboratoryjnych, co może prowadzić do błędnych predykcji. Dzięki selekcji cech możemy skupić się na najważniejszych zmiennych, takich jak poziom glukozy czy ciśnienie krwi, ignorując mniej istotne dane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykłady w praktyce: Jak przeprowadzić selekcję cech?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korelacja dla zmiennych ciągłych\n",
    "\n",
    "Obliczenie współczynnika korelacji Pearson dla każdej cechy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja `pearsonr` oblicza współczynnik korelacji Pearsona pomiędzy wybraną cechą a `y`. Nasze `corr` to współczynnik korelacji (wartość od -1 do 1), gdzie:\n",
    "* Wartości bliskie 1 (korelację) lub -1 (odwrotną korelację) oznaczają silną zależność.\n",
    "* Wartość bliska 0 oznacza brak związku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korelacja dla age: 0.19\n",
      "Korelacja dla sex: 0.04\n",
      "Korelacja dla bmi: 0.59\n",
      "Korelacja dla bp: 0.44\n",
      "Korelacja dla s1: 0.21\n",
      "Korelacja dla s2: 0.17\n",
      "Korelacja dla s3: -0.39\n",
      "Korelacja dla s4: 0.43\n",
      "Korelacja dla s5: 0.57\n",
      "Korelacja dla s6: 0.38\n"
     ]
    }
   ],
   "source": [
    "for i, feature in enumerate(data.feature_names):\n",
    "    corr, _ = pearsonr(X[:, i], y)\n",
    "    print(f\"Korelacja dla {feature}: {corr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cechy posortowane według wartości korelacji:\n",
      "bmi: 0.59\n",
      "s5: 0.57\n",
      "bp: 0.44\n",
      "s4: 0.43\n",
      "s3: -0.39\n",
      "s6: 0.38\n",
      "s1: 0.21\n",
      "age: 0.19\n",
      "s2: 0.17\n",
      "sex: 0.04\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Oblicz korelacje dla każdej cechy\n",
    "correlations = []\n",
    "for i, feature in enumerate(data.feature_names):\n",
    "    corr, _ = pearsonr(X[:, i], y)\n",
    "    correlations.append((feature, corr))\n",
    "\n",
    "# Posortuj cechy według wartości korelacji (malejąco)\n",
    "correlations_sorted = sorted(correlations, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Wyświetl posortowane korelacje\n",
    "print(\"Cechy posortowane według wartości korelacji:\")\n",
    "for feature, corr in correlations_sorted:\n",
    "    print(f\"{feature}: {corr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recursive Feature Elimination (RFE)** to metoda selekcji cech, która iteracyjnie usuwa najmniej istotne cechy z zestawu danych, aż zostanie osiągnięta pożądana liczba cech. Proces opiera się na wykorzystaniu modelu uczącego się do oceny istotności cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RFE` wymaga modelu uczącego się, który oceni znaczenie cech. Może to być dowolny model, który potrafi zwracać miary ważności cech (np. `RandomForestRegressor`, `LinearRegression`, `SVC` z funkcją `coef_` lub `feature_importances_`).\n",
    "Iteracyjny proces:\n",
    "1. RFE trenuje model na danych i ocenia wszystkie cechy.\n",
    "2. Następnie usuwa cechę o najniższej istotności.\n",
    "3. Proces powtarza się aż do osiągnięcia określonej liczby cech (n_features_to_select).\n",
    "Jako wynik `RFE` zwraca zestaw danych z wybranymi cechami oraz listę wskazującą, które cechy zostały wybrane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(model, n_features_to_select=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RFE(estimator=RandomForestRegressor(random_state=42), n_features_to_select=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RFE</label><div class=\"sk-toggleable__content\"><pre>RFE(estimator=RandomForestRegressor(random_state=42), n_features_to_select=5)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RFE(estimator=RandomForestRegressor(random_state=42), n_features_to_select=5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wybrane cechy: ['bmi', 'bp', 's2', 's5', 's6']\n"
     ]
    }
   ],
   "source": [
    "selected_features = [data.feature_names[i] for i in range(len(data.feature_names)) if rfe.support_[i]]\n",
    "print(\"Wybrane cechy:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LassoCV` to regresja liniowa, która wykorzystuje regularizację L1. Regularizacja L1 dodaje karę za nieistotne współczynniki, ustawiając je na zero, co skutkuje automatyczną selekcją cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(cv=5).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wybrane cechy przez Lasso: ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's4', 's5', 's6']\n"
     ]
    }
   ],
   "source": [
    "selected_features = [data.feature_names[i] for i in range(len(data.feature_names)) if lasso.coef_[i] != 0]\n",
    "print(\"Wybrane cechy przez Lasso:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (Principal Component Analysis) – Analiza głównych składowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA** to metoda redukcji wymiarowości danych. Przekształca oryginalne cechy w nowy zestaw cech zwanych głównymi składowymi, które są liniowymi kombinacjami oryginalnych cech. Główne składowe są uporządkowane w taki sposób, że pierwsza składowa wyjaśnia największą możliwą wariancję w danych, druga składowa wyjaśnia kolejną największą wariancję (prostopadle do pierwszej), i tak dalej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jak działa PCA?\n",
    "1. **Standaryzacja danych:** Dane są standaryzowane (średnia = 0, odchylenie standardowe = 1), aby różnice w skalach cech nie wpłynęły na wyniki.\n",
    "2. **Macierz kowariancji:** Obliczana jest macierz kowariancji, aby uchwycić zależności między cechami.\n",
    "3. **Wartości własne i wektory własne:** Obliczane są wartości własne i wektory własne macierzy kowariancji. Wektory własne reprezentują kierunki (składowe główne), a wartości własne wskazują, jak dużo wariancji jest wyjaśniane przez każdą składową.\n",
    "4. **Projekcja danych:** Dane są rzutowane na nowe osie określone przez wektory własne. W ten sposób tworzy się nowy zestaw cech (główne składowe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dlaczego używamy PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Redukcja wymiarowości:** Usunięcie mniej znaczących cech przy zachowaniu jak największej ilości informacji.\n",
    "* **Wizualizacja:** Umożliwia przedstawienie danych w 2D lub 3D.\n",
    "* **Usuwanie współliniowości:** Nowe cechy są niezależne od siebie (ortogonalne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Załaduj dane\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = (data.target > 140).astype(int)  # Przekształcamy target na binarny (1 dla chorych, 0 dla zdrowych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standaryzacja danych\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA - redukcja do 4 głównych składowych\n",
    "pca = PCA(n_components=4)\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pole `explained_variance_ratio_` w obiekcie `pca` wyświetla, jaka część wariancji została wyjaśniona przez każdą główną składową:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40242108, 0.14923197, 0.12059663, 0.09554764])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wariancja wyjaśniona przez składowe PCA: [0.40242108 0.14923197 0.12059663 0.09554764]\n",
      "Łączna wariancja wyjaśniona: 0.7677973090140596\n"
     ]
    }
   ],
   "source": [
    "# Wyświetl wariancję wyjaśnioną przez każdą składową\n",
    "print(\"Wariancja wyjaśniona przez składowe PCA:\", pca.explained_variance_ratio_)\n",
    "print(\"Łączna wariancja wyjaśniona:\", sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność modelu na danych PCA: 0.7579\n"
     ]
    }
   ],
   "source": [
    "# Model na danych zredukowanych przez PCA\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "score = cross_val_score(model, X_pca, y, cv=5, scoring=\"accuracy\")\n",
    "print(f\"Dokładność modelu na danych PCA: {np.mean(score):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność modelu na oryginalnych danych: 0.6853\n"
     ]
    }
   ],
   "source": [
    "# Model na oryginalnych danych (dla porównania)\n",
    "score_original = cross_val_score(model, X_scaled, y, cv=5, scoring=\"accuracy\")\n",
    "print(f\"Dokładność modelu na oryginalnych danych: {np.mean(score_original):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kiedy selekcja cech nie jest potrzebna?\n",
    "Nie zawsze konieczne jest stosowanie selekcji cech. W przypadku modeli takich jak drzewa decyzyjne, Random Forest czy XGBoost, które mają wbudowaną zdolność do ignorowania mniej istotnych cech, selekcja może być mniej istotna. Jednak nawet w takich przypadkach ograniczenie liczby cech może przyspieszyć proces treningu i poprawić interpretowalność wyników."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 1:** Porównnaj działanie PCA (n_components = 5), LassoCV (cv = 5), RFE (n_features_to_select=5) i Korelacji Persona na jednym wybranym innym niż `diabetes` zbiorze danych. Użyj modelu `RandomForestClassifier`. Pamiętaj, ze niektóre dane przed nakarmieniem modelu będzie trzeba odpoiwendio przygotować.\n",
    "\n",
    "**Zadanie 2:** Załaduj zbiór danych `iris`, `breast_cancer` oraz jeden wybrany przez siebie. Przygotuj algorytm, który znajdzie taką ilośc cech, aby dokładność modelu `RandomForestClassifier` była jak najwyższa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
